{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c28aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yazılım alanı\\AppData\\Local\\Temp\\ipykernel_3244\\3075349580.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pa\n"
     ]
    }
   ],
   "source": [
    "import numpy as nm\n",
    "import pandas as pa\n",
    "import re\n",
    "dataSetPath = r\"tatoeba-dataset_Edited712k.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d66a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İçeri aktarılan cümle sayısı: 712143\n"
     ]
    }
   ],
   "source": [
    "# Veri setinin içeri aktarılması:\n",
    "dataEng = []\n",
    "dataTR = []\n",
    "with open(dataSetPath, 'r', encoding='utf8') as file:\n",
    "    for line in file:\n",
    "        parts = line.split('\\t')\n",
    "        dataEng.append(parts[1])\n",
    "        dataTR.append(parts[3])\n",
    "print(f\"İçeri aktarılan cümle sayısı: {len(dataTR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc416d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verinin sıralanması ve uzun cümlelerin veri setinden çıkartılması:\n",
    "\n",
    "# Verinin kelîme uzunluğuna göre sıralanması:\n",
    "data = {\"TR\": dataTR, \"ENG\": dataEng}\n",
    "dfData = pa.DataFrame(data)\n",
    "dfData[\"len\"] = dfData[\"ENG\"].apply(lambda x: len(x.split()))\n",
    "dfData.sort_values(\"len\", ascending = True, inplace = True)\n",
    "\n",
    "# Veri seti kısa metînlerden oluşuyor; fakat 190 kelîme uzunluğunda metînler de var. Metîn uzunluğunun sâbit olması lazım\n",
    "# 12 kelîme seçildiğinde verilerin %95'inden fazlasını alabiliyoruz:\n",
    "dfData = dfData[dfData[\"len\"] <= 12]\n",
    "\n",
    "dfData.drop(\"len\", inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b87b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazla boşlukların, yeni satır karakterlerin cümlelerden çıkartılması:\n",
    "dfData[\"ENG\"] = dfData[\"ENG\"].str.strip()\n",
    "dfData[\"TR\"] = dfData[\"TR\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f60df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Küçük harfe çevirme:\n",
    "# Özel isimler de küçük harfe çevriliyor;\n",
    "# bunu önlemek için bu işlemden evvel özel isimleri NER (Named Entity Recognition) ile işâretlemek iyi bir fikir olabilir;\n",
    "# fakat şu an için bunu yapmamız zâten karmaşık olan işi daha karmaşık hâle getirir.\n",
    "dfData[\"ENG\"] = dfData[\"ENG\"].apply(lambda x: x.lower())\n",
    "lowerTR = lambda x: x.replace('I', 'ı').lower()\n",
    "dfData[\"TR\"] = dfData[\"TR\"].apply(lowerTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b03fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Türkçe çevirilerde sıklıkla rastlanan bir yazım hatâsı: Allahın -> Allah'ın şeklinde düzeltilmeli\n",
    "\n",
    "pattern = re.compile(r\"Allah\\w+\")\n",
    "dfData.reset_index(drop=True, inplace = True)\n",
    "foundPat = dfData[\"TR\"].apply(lambda x: re.search(pattern, x) is not None)\n",
    "for i in range(0, len(foundPat)):\n",
    "    if(foundPat[i]):\n",
    "        dfData[\"TR\"].iloc[i] = dfData[\"TR\"].iloc[i].replace(r\"Allah\", r\"Allah'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e56ce70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Veri arttırımı (data augmentation)\n",
    "# Veri içerisinde \"he's\", \"I'm\", \"doesn't\" gibi kısaltmalar var.\n",
    "# Bunların gerçekte \"he is\", \"I am\", \"does not\" olduğunun öğrenilmesi için\n",
    "# bunların bir kısmının uzun hâlini de veri olarak veri setine eklemek iyi bir fikir olabilir.\n",
    "# \"he's\" kelîmesinin anlamının \"he is\" olduğu daha iyi modellenebilir.\n",
    "# Veri arttırımı için örüntüler:\n",
    "pat = re.compile(r\"\\w*'re\")# 're -> (you | they | we) are\n",
    "pat2 = re.compile(r\"\\w{1,4}'ve\")# 've -> (I | you | they | we) have\n",
    "pat3 = re.compile(r\"i'm\")# i'm -> I am\n",
    "pat4 = re.compile(r\"\\w{1, 4}'ll\")# (I | you | we | they | he | she | it)'ll -> will\n",
    "pat5 = re.compile(r\"doesn't\")# doesn't -> does not\n",
    "pat6 = re.compile(r\"isn't\")# isn't -> is not\n",
    "pat7 = re.compile(r\"aren't\")# aren't -> are not\n",
    "pat8 = re.compile(r\"won't\")# won't -> will not\n",
    "pat9 = re.compile(r\"couldn't\")# couldn't -> could not\n",
    "pat10 = re.compile(r\"wouldn't\")# wouldn't -> would not\n",
    "pat11 = re.compile(r\"wasn't\")# wasn't -> was not\n",
    "pat12 = re.compile(r\"were't\")# were't -> were not\n",
    "pat13 = re.compile(r\"haven't\")# haven't -> have not\n",
    "pat14 = re.compile(r\"hasn't\")# hasn't -> has not\n",
    "pat15= re.compile(r\"hadn't\")# hadn't -> had not\n",
    "pat16 = re.compile(r\"\\w{1,4}'d\")# [I | you | we | they | he | she | it]'d -> .. had\n",
    "pat17 = re.compile(r\"don't\")# don't -> do not\n",
    "pat18 = re.compile(r\"\\w{1,3}'s been\")# 's been -> 's has been\n",
    "pat19 = re.compile(r\"\\w{1,3}'s (?!been)\")# (he | she | it)'s -> .. is (been içermeyen) \n",
    "\n",
    "# 's takısı pek çok anlama gelebilmektedir. Sâhiplik olan ek ile karışmaması için sadece zamirlere uygulanmalı; fakat\n",
    "# zamire gelen 's takısı 'has' yardımcı fiiline de işâret edebilmektedir.\n",
    "# Bunu ayırt etmek için sonrasında 'been' ifâdesinin gelip, gelmediğine bakmalıyız:\n",
    "# he's a doctor -> 'be' fiili -> he is\n",
    "# he's been working for 5 hour -> 'has' -> he has\n",
    "# BU OLMAZ: pat8 = re.compile(r\"she's|it's|he's\")# he's, He's, she's, She's, It's, it's -> .. is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a279f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verilen örüntüyü verilen veri içerisinde arayıp, belirtilen dönüşümlere göre veri üretimi yapan fonksiyon:\n",
    "def applyPattern(dataAsDataFrame, pattern, fromThatToApplyThisDict, applyToOdd: bool = False, applyToEven: bool = True):\n",
    "    \"\"\"\n",
    "    Verilen veri DataFrame formatında olmalı\n",
    "    İngilizce veriler \"ENG\" sütununda, Türkçe veriler \"TR\" sütununda yer almalı\n",
    "    Geriye arttırılan veriler liste olarak döndürülür; DataFrame'e kaydedilmez\n",
    "    Geriye döndürülen listenin her elemanı bir sözlüktür; bu sözlükte \"ENG\" anahtarında İngilizce cümle, \"TR\" anahtarında\n",
    "    Türkçe cümle yer almaktadır\n",
    "    Verilerin indisleri 0, 1, 2, 3... şeklinde sıralı şekilde olmalıdır\n",
    "    \"\"\"\n",
    "    newData = []\n",
    "    foundPat = dataAsDataFrame[\"ENG\"].apply(lambda x: re.search(pattern, x) is not None)\n",
    "    for i in range(0, len(foundPat)):#dataAsDataFrame[\"ENG\"][foundPat == True]:\n",
    "        if not foundPat.iloc[i]:\n",
    "            continue\n",
    "        even = (i % 2 == 0)\n",
    "        if (applyToOdd and not even) or (applyToEven and even):\n",
    "            sent = dataAsDataFrame[\"ENG\"].iloc[i]\n",
    "            for key in fromThatToApplyThisDict:\n",
    "                sent = sent.replace(key, fromThatToApplyThisDict[key])\n",
    "            engTRDict = {}\n",
    "            engTRDict[\"ENG\"] = sent\n",
    "            engTRDict[\"TR\"] = dataAsDataFrame[\"TR\"].iloc[i]\n",
    "            newData.append(engTRDict)\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8bedd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri arttırımı için gerekli dönüşüm verileri:\n",
    "patternData = {\"you're\" : \"you are\", \"they're\" : \"they are\", \"we're\" : \"we are\"}\n",
    "patData2 = {\"i've\": \"i have\", \"you've\": \"you have\", \"they've\": \"they have\", \"we've\": \"we have\"}\n",
    "patData3 = {\"i'm\": \"i am\"}\n",
    "patData4 = {\"i'll\": \"i will\", \"you'll\": \"you will\", \"he'll\": \"he will\", \"she'll\": \"she will\", \"it'll\": \"it will\",\n",
    "            \"we'll\": \"we will\", \"they'll\": \"they will\"}\n",
    "patData5 = {\"doesn't\": \"does not\"}\n",
    "patData6 = {\"isn't\": \"is not\"}\n",
    "patData7 = {\"aren't\": \"are not\"}\n",
    "patData8 = {\"won't\": \"will not\"}\n",
    "patData9 = {\"couldn't\": \"could not\"}\n",
    "patData10 = {\"wouldn't\": \"would not\"}\n",
    "patData11 = {\"wasn't\": \"was not\"}\n",
    "patData12 = {\"weren't\": \"were not\"}\n",
    "patData13 = {\"haven't\": \"have not\"}\n",
    "patData14 = {\"hasn't\": \"has not\"}\n",
    "patData15 = {\"hadn't\": \"had not\"}\n",
    "patData16 = {\"i'd\": \"i had\", \"you'd\": \"you had\", \"we'd\": \"we had\", \"he'd\": \"he had\", \"she'd\": \"she had\", \"it'd\": \"it had\",\n",
    "            \"they'd\": \"they had\"}\n",
    "patData17 = {\"don't\": \"do not\"}\n",
    "patData18 = {\"he's\": \"he has\", \"she's\": \"she has\", \"it's\": \"it has\"}\n",
    "patData19 = {\"she's\": \"she is\", \"he's\": \"he is\", \"it's\": \"it is\"}\n",
    "\n",
    "# Belirtilen dönüşümler, ilgili örüntünün bulunduğu cümlelerin yarısına (çift indisli olanlarına) uygulanmaktadır:\n",
    "patterns = [pat, pat2, pat3, pat4, pat5, pat6, pat7, pat8, pat9, pat10, pat11, pat12, pat13, pat14, pat15, pat16]\n",
    "patAllData = [patternData, patData2, patData3, patData4, patData5, patData6, patData7, patData8, patData9, patData10,\n",
    "             patData11, patData12, patData13, patData14, patData15, patData16]\n",
    "allTogether = dict(zip(patterns, patAllData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b2ccd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eklenen yeni cümle sayısı: 12892\n"
     ]
    }
   ],
   "source": [
    "# Veri arttırımı: Hâzırladığımız örüntülerle 100 bine yakın yeni veri çıkartılabiliyor; bunu kısıtlamak için her birinden\n",
    "# bin adet veri alınabilir\n",
    "newData = []\n",
    "for pattern, patternData in allTogether.items():\n",
    "    produced = applyPattern(dfData, pattern, patternData)\n",
    "    for i in range(0,len(produced)):\n",
    "        newData.append(produced[i])\n",
    "        if i == 1000:\n",
    "            break\n",
    "print(f\"Eklenen yeni cümle sayısı: {len(newData)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee718e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri seti nihâî boyutu: 703931\n"
     ]
    }
   ],
   "source": [
    "# Yeni verilerin veri setine eklenmesi:\n",
    "#print(f\"Üretilen yeni cümle sayısı: {len(newData)}\")\n",
    "dfNew = pa.DataFrame(columns = [\"ENG\", \"TR\"], data = newData)\n",
    "dfData = pa.concat([dfData, dfNew])\n",
    "print(f\"Veri seti nihâî boyutu: {len(dfData)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7107ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeni verilerin hazineye eklenmesinden sonra verileri yeniden sıralama:\n",
    "dfData[\"len\"] = dfData[\"ENG\"].apply(lambda x: len(x.split()))\n",
    "dfData.sort_values(\"len\", ascending = True, inplace = True)\n",
    "dfData.drop(\"len\", inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc62b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verinin dışa aktarılması:\n",
    "dfData.to_csv(\"cleaned703KSentences.csv\", encoding = \"utf8\", index = False, sep = '\\t')\n",
    "# Cümlelerde virgül olabilir; bu sebeple sep parametresine '\\t' gibi metîn içerisinde olmayan bir karakter verilmeli"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
