{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333a9e54",
   "metadata": {
    "executionInfo": {
     "elapsed": 3947,
     "status": "ok",
     "timestamp": 1751171072701,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "1d9dad3e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer# Kelîmeleri simgeleştirmek için\n",
    "from keras.layers import Dense, GRU, Input, Embedding# Sinir ağı katmanları\n",
    "from tensorflow.keras.models import Model# Fonksiyonel API kullanarak modeli oluşturmak için\n",
    "from keras.callbacks import ModelCheckpoint# Sinir ağının eğitim sırasında kaydedilmesi için\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences# Cümleleri eşit uzunluğa getirmek için\n",
    "from keras.losses import SparseCategoricalCrossentropy# Kayıp fonksiyonuyla ilgili detaylar için\n",
    "from keras.optimizers import RMSprop# RMSProp yinelenen sinir ağlarında daha başarılı\n",
    "import numpy as nm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e883ff1",
   "metadata": {
    "id": "426a124a"
   },
   "source": [
    "### Veri seti ve kullanılan dil modelleri hakkında:\n",
    "- Veri seti : Çeviri için akla gelen ilk veri seti kaynaklarından birisi tatoeba.org adresidir. Buradaki nisbeten kısa cümleler; fakat şu an mükemmel bir sistem tasarlamaya çalışmadığımız için ve veri büyüklüğü iyi seviyede olduğu için kullanılabilir. Veri seti, https://tatoeba.org/tr/downloads adresinden indirilmiştir. Websitesinin anasayfasının adresi https://tatoeba.org/tr 'dir. Cümleler CC-BY lisansı ve bu lisansın farklı versiyonları altında sunulmaktadır.\n",
    "- İngilizce kelîme vektörü veri seti : Stanford Üniversitesi'nin hâzırladığı GLoVe kelîme vektör modeli (https://github.com/stanfordnlp/GloVe?tab=readme-ov-file), kamu malı tahsis lisansı (Open Data Commons Public Domain Dedication and License (PDDL)) altında sunulmaktadır.\n",
    "- Türkçe kelîme vektörü veri seti : MIT lisansıyla sunulan, eğitilmiş bir Word2Vec kelîme vektör modeli (https://github.com/akoksal/Turkish-Word2Vec?tab=readme-ov-file)\n",
    "- Aslında, veri setini Embedding katmanıyla gömüp, ağırlıkların eğitim süresince değişebilir olmasını sağlamak da kâfî olabilir; fakat daha kaliteli bir model için Embedding katmanlarının ağırlıklarını ilgili dil için hâzırlanmış bir kelîme vektörüyle temsil etmek lazım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5424fe",
   "metadata": {
    "id": "de009181"
   },
   "source": [
    "### Proje detayları:\n",
    "- Uçtan uca (sequence to sequence) bir makine çevirisi ağı oluşturmak istiyoruz.\n",
    "- Bunun için iki yapı kullanmalıyız: encoder (kodlayıcı) ve decoder (kod çözücü)\n",
    "- Kodlayıcı, kaynak dildeki cümleyi alır ve bir düşünce matrisi oluşturur\n",
    "- Kod çözücü bu düşünce matrisini alır ve hedef dile çeviri yapar.\n",
    "- Eğitimde, kod çözücüye hem düşünce matrisini, hem de hedef çevirinin ilk kelîmesini vermemiz gerekiyor; böylece modelin sonraki kelîme doğru tahmîn etmeyi öğrenmesini bekliyoruz. Elbette bunun için geri yayılım uygulayan sinir ağına ihtiyacımız var.\n",
    "- Kod çözücü veyâ çözümleyici kaynak cümleyi hedef cümleye çevirirken sonraki kelîmeyi tahmîn ederek devâm etmektedir; çözümleyicinin cümlenin nerede başlayıp, nerede biteceğini tahmîn edebilmesi için cümle başını ve sonunu bir 'token' olarak işâretlememiz gerekiyor. Bunun için metîn içerisinde geçmeyen özel bir karakter dizisi (`<START>` ve `<END>` gibi) seçmeliyiz; fakat biz '<' ve '>' karakterlerini veri setinden çıkarmak istediğimiz için 'ssss ' ve ' eeee' seçebiliriz.\n",
    "- Başlangıç karakteri olan 'ssss ' ifâdesinin sonunda, bitiş karakteri olan ' eeee' ifâdesinin başında boşluk karakteri olduğuna dikkat ediniz. Bu, bu kelîmelerin diğer kelîmelerle birleşmemesi için şarttır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9513efc",
   "metadata": {
    "id": "b4f11edd"
   },
   "source": [
    "- Veri ön işleme işlemi yapıldı. Bir kısım düzeltme elle yapıldı (uygunsuz cümle silme, çeviri düzeltme vs.);\n",
    "- fakat asıl veri ön işleme işlemi adımları 'VeriOnIsleme.ipynb' dosyasında mevcuttur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92e0e59",
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1751171076741,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "b53d94ae"
   },
   "outputs": [],
   "source": [
    "# Hiperparametreler:\n",
    "\n",
    "# 'Modeli eğit' bayrağı:\n",
    "trainModel = False\n",
    "\n",
    "# Veri seti hiperparametreleri:\n",
    "useAllDataSet = False# Tüm veri setini kullan = True\n",
    "limitCountOfSentences = 400000# Tüm veri setini kullanılmayacaksa, cümle sayısı\n",
    "\n",
    "# Tokenizer ve Embedding hiperparametreleri:\n",
    "embeddingSize = 100# Kelîme gömme boyutu\n",
    "useManuelMaxLengthForSentences = False# Cümle uzunluğunu elle kısaltmak için bu seçeneği 'True' olarak bırakın\n",
    "# Yukarıdaki ayarı 'False' olarak belirlerseniz, veri setindeki en uzun cümle, azamî cümle uzunluğu olarak seçilir.\n",
    "maxLengthForTRSentences = 9# Türkçe metînler için (kod çözücü) cümle uzunluğu\n",
    "maxLengthForENGSentences = 8# Türkçe metînler için (kod çözücü) cümle uzunluğu\n",
    "loadEmbeddingWeightsFromFiles = True# Ağırlıkları bir kez kaydettikten sonra hız ve bellek kazanımı için bu parametreyi açın\n",
    "# Eğer içeri aktardığınız cümle sayısı değişirse, yukarıdaki parametreyi 'False' yapın\n",
    "\n",
    "# Model hiperparametreleri:\n",
    "gruCellSize = 196# GRU katmanlarının (6 adet) hücre sayısı (İşlem gücünüz varsa, 256 veyâ daha fazla verin)\n",
    "epochNum = 10# Tur ('epoch') sayısı\n",
    "batchSizeNum = 90# İşlem gücünüz varsa 96, 128, 256 ve daha fazlasını seçebilirsiniz\n",
    "learningRate = 0.004# RMSProp optimizasyon algoritması için öğrenme katsayısı (hızı)\n",
    "\n",
    "# Modeli yeniden eğitirken model ağırlıklarının dosyadan yüklenmesi için bu ayarı True yapın:\n",
    "loadModelWeights = True# Model ağırlığı 'modelSavePath' isimli değişkendeki dosya yolundaki dosyadan alınır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7893f73",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1751171079925,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "1c10aee9"
   },
   "outputs": [],
   "source": [
    "# Dosya yolları:\n",
    "trModelPath = r\"trmodel\"# Türkçe dil modeli\n",
    "engModelPath = r\"glove.6B.100d.txt\"# İngilizce dil modeli\n",
    "dataSetPath = r\"cleaned703KSentences.csv\"\n",
    "\n",
    "modelSavePath = 'nmt_v2.weights.h5'# Model ağırlığının kaydedilmesinde ve / veyâ geri yüklenmesinde kullanılır\n",
    "\n",
    "# Ağırlıkları dosyadan yüklemek için numpy dizilerinin dosya yolu:\n",
    "additionalPart = ('allSentences' if useAllDataSet else str(limitCountOfSentences) + 'Sentences' + str(embeddingSize) + 'D')\n",
    "# Yukarıdaki kod, farklı embeddingSize ve cümle sayısı seçildiğinde ağırlıkların farklı dosyalara kaydedilmesi içindir.\n",
    "weightsOfEmbeddingEncoderPath = 'weightsOfEmbeddingEncoder'+ additionalPart + '.dat'\n",
    "weightsOfEmbeddingDecoderPath = 'weightsOfEmbeddingDecoder' + additionalPart + '.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82ffe1a",
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1751171182266,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "5665706b"
   },
   "outputs": [],
   "source": [
    "# Farklı uzunluktaki cümleler için kelîme vektörleri dil modelinden alınmalı:\n",
    "if not loadEmbeddingWeightsFromFiles:\n",
    "    from gensim.models import Word2Vec# Dil modeli\n",
    "    from gensim.models import KeyedVectors# Word2Vec dil modelini içe aktarmak için"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b7c6f85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2106,
     "status": "ok",
     "timestamp": 1751171272219,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "0a29a4f2",
    "outputId": "12f7cb16-c2f6-482f-b341-beba8a03363d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İçeri aktarılan cümle sayısı: 400000\n"
     ]
    }
   ],
   "source": [
    "# Veri setinin içeri aktarılması:\n",
    "sayac = 0\n",
    "dataEng = []\n",
    "dataTR = []\n",
    "with open(dataSetPath, 'r', encoding='utf8') as file:\n",
    "    for line in file:\n",
    "        if sayac == 0:\n",
    "            sayac += 1\n",
    "            continue\n",
    "        parts = line.split(\"\\t\")\n",
    "        dataTR.append(parts[0].strip())\n",
    "        dataEng.append(parts[1].strip())\n",
    "        if not useAllDataSet:\n",
    "            if sayac == limitCountOfSentences:\n",
    "                break\n",
    "            sayac += 1\n",
    "print(f\"İçeri aktarılan cümle sayısı: {len(dataTR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2f84c4",
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1751171278458,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "b549c43b"
   },
   "outputs": [],
   "source": [
    "# Dil modellerinin içe aktarılması:\n",
    "if not loadEmbeddingWeightsFromFiles:\n",
    "    modelTR = KeyedVectors.load_word2vec_format(trModelPath, binary = True)\n",
    "    try:\n",
    "        modelEng = KeyedVectors.load_word2vec_format(engModelPath + \".word2vec\", binary = False)\n",
    "    except:\n",
    "        from gensim.scripts.glove2word2vec import glove2word2vec# GLoVe dil modelini Word2Vec formatına çevirmek için\n",
    "        glove2word2vec(engModelPath, engModelPath + \".word2vec\")# İngilizce modeli word2vec formatına çevirir\n",
    "        #ve kaynak dizinde '.word2vec' uzantısıyla kaydeder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349757d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1751171280979,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "16bc6a86",
    "outputId": "9f1f6d0e-71d1-4d0b-f370-b1f7957025dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssss harika! eeee\n"
     ]
    }
   ],
   "source": [
    "# Başlama ve bitiş simgelerinin hedef cümlelere (Türkçe cümlelere) eklenmesi:\n",
    "startToken = \"ssss \"\n",
    "endToken = \" eeee\"\n",
    "dataTR = [startToken + i + endToken for i in dataTR]\n",
    "print(dataTR[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72adf646",
   "metadata": {
    "id": "eb60579a"
   },
   "source": [
    "- Tokenizer (Simgeleştirici) ile verileri simgeleştirmeliyiz; fakat mevcut Tokenizer'da eksik olan metni ters çevirme ve simge dizisinden metne çevirme gibi özellikleri eklemeliyiz.\n",
    "- Ayrıca bu işlemden sonra uygulanan `pad_sequences()` yöntemini de buradan çalıştırmak ve hangi yapı için hangi parametreleri verdiğimizi daha kolay görmek için bu işlemi de alt sınıflayarak `Tokenizer` bünyesine katabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fda5477",
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1751171284321,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "0a83d102"
   },
   "outputs": [],
   "source": [
    "# TokenizerExtended tanımı:\n",
    "class TokenizerExtended(Tokenizer):\n",
    "    def __init__(self, texts, num_words = None, reverse = False, truncating = True, padding = 'pre',\n",
    "                maxLengthOfSentence = None):\n",
    "        Tokenizer.__init__(self, num_words = num_words)\n",
    "        self.fit_on_texts(texts)\n",
    "        self.sequences = self.texts_to_sequences(texts)\n",
    "        if reverse:# Metni ters çevir, metnin sonundan kes; başı daha mühim\n",
    "            texts = [sent[::-1] for sent in texts]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "        if maxLengthOfSentence is None:# Azamî uzunluk belirtilmemişse..\n",
    "            lens = nm.asarray([len(i) for i in self.sequences])\n",
    "            self.maxToken = lens.max()\n",
    "        else:\n",
    "            self.maxToken = maxLengthOfSentence\n",
    "        self.sequencesPadded = pad_sequences(self.sequences, maxlen = self.maxToken,\n",
    "                                             truncating=truncating, padding=padding)\n",
    "\n",
    "    def tokenToWord(self, token):\n",
    "        try:\n",
    "            return ' ' if token == 0 else self.index_word[token]\n",
    "        except:# Simgeleyicini sözlüğünde olmayan bir simge verildiyse, boşluk karakterini döndür\n",
    "            return ' '\n",
    "\n",
    "    def sentenceToSequences(self, sentence, reverse = False, padding = 'post'):\n",
    "        sentence = sentence\n",
    "        if reverse:\n",
    "            sentence = \" \".join(sentence.split()[::-1])\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "        seq = self.texts_to_sequences([sentence])\n",
    "        seqPadded = pad_sequences(seq, maxlen = self.maxToken, truncating = truncating, padding = padding)\n",
    "        return nm.asarray(seqPadded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137ff6c9",
   "metadata": {
    "executionInfo": {
     "elapsed": 17901,
     "status": "ok",
     "timestamp": 1751171305794,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "f4e8a99d"
   },
   "outputs": [],
   "source": [
    "# Veri format tamâmlama:\n",
    "if useManuelMaxLengthForSentences:\n",
    "    tokenizerEncoder = TokenizerExtended(dataEng, padding='pre', reverse=True, truncating='pre',\n",
    "                                     maxLengthOfSentence = maxLengthForENGSentences)\n",
    "    tokenizerDecoder = TokenizerExtended(dataTR, padding='post', reverse=False, truncating='post',\n",
    "                                     maxLengthOfSentence = maxLengthForTRSentences)\n",
    "else:\n",
    "    tokenizerEncoder = TokenizerExtended(dataEng, padding='pre', reverse=True, truncating='pre')\n",
    "    tokenizerDecoder = TokenizerExtended(dataTR, padding='post', reverse=False, truncating='post')\n",
    "# Kodlayıcı cümlelerinin ters çevrilmesinin sebebi verimliliktir.\n",
    "# Metîn uzunluğunu 12 olarak belirledik; çünkü bu, göstermelik bir model ve metînlerimiz gâyet kısa.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfd98e8",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1751171308244,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "8a23adcb"
   },
   "outputs": [],
   "source": [
    "# Kısayollar:\n",
    "sequencesSrc = tokenizerEncoder.sequencesPadded\n",
    "sequencesTarget = tokenizerDecoder.sequencesPadded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ecbbe87",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751171310521,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "37d5e0a2"
   },
   "outputs": [],
   "source": [
    "# Dil modellerinin kelîme vektör boyutunu istediğimiz boyuta, anlamı fazla kaybetmeden dönüştürmek için bir fonksiyon:\n",
    "from math import ceil\n",
    "def shrinkWordVectors(word2VecModel, targetDimensionLength, sourceTokenizer: Tokenizer, asList = False):\n",
    "    \"\"\"\n",
    "        Bu, eğitilmiş dil modellerinin vektör boyutunu anlamı fazla kaybetmeden küçültmek için bir fonksiyondur.\n",
    "        Eğer sadece dil modelinden ilgili ağırlıkların liste veyâ sözlük olarak alınması isteniyorsa (boyut korunarak)\n",
    "        targetDimensionLength parametresine ilgili dil modelinin boyut uzunluğu verilmelidir.\n",
    "        word2VecModel: 'gensim.models.KeyedVectors' tipinde olmalıdır\n",
    "        targetDimensionLength değeri verilen modelin vektör boyutundan küçük olmalıdır.\n",
    "        Eğer kaynağın boyut uzunluğundan daha büyük bir sayı `targetDimensionLength` olarak verilirse `None` döndürülür\n",
    "        sourceTokenizer: Verilen modeldeki tüm kelîmeler yerine sadece verilen 'Tokenizer' nesnesindeki kelîmelerin\n",
    "        hedef boyuta uygun vektörü isteniyorsa, bu parametre sağlanmalıdır.\n",
    "        asList : Eğer bir 'Tokenizer' sağlanırsa ve ağırlıklar Tokenizer'ın kelîmelerinin sırasıyla dizilmiş bir liste olarak\n",
    "        istenirse, bu parametreye `True` değeri verilmelidir.\n",
    "\n",
    "        'Tokenizer' 0 indisini kullanmadığından dizi olarak dönen değer len(tokenizer.index_word) + 1 olmaktadır.\n",
    "    \"\"\"\n",
    "    embeddingSize = word2VecModel.vector_size\n",
    "    if embeddingSize < targetDimensionLength:\n",
    "        return None\n",
    "    stepSize = ceil((embeddingSize / targetDimensionLength))\n",
    "    newWordToIndex = {}\n",
    "    if sourceTokenizer is not None:\n",
    "        newEmbeddings = nm.random.uniform(low = -1, high = 1, size=(len(sourceTokenizer.word_index) + 1, targetDimensionLength))\n",
    "        for word, sayac in sourceTokenizer.word_index.items():\n",
    "            try:\n",
    "                vect = word2VecModel.get_vector(word)\n",
    "                for i in range(0, targetDimensionLength):\n",
    "                    newEmbeddings[sayac][i] = vect[i*stepSize:(i+1)*stepSize].sum()\n",
    "                if not asList:\n",
    "                    newWordToIndex[word] = newEmbeddings[sayac]\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        words = modelTR.index_to_key\n",
    "        newEmbeddings = {}\n",
    "        if word in words:\n",
    "            try:\n",
    "                vect = word2VecModel.get_vector(word)\n",
    "                newVect = nm.zeros(targetDimensionLength)\n",
    "                for i in range(0, targetDimensionLength):\n",
    "                    newVect[i] = vect[i*stepSize:(i+1)*stepSize].sum()\n",
    "                newEmbeddings[word] = newVect\n",
    "            except:\n",
    "                pass\n",
    "    if (sourceTokenizer is not None) and (not asList):\n",
    "        return newWordToIndex\n",
    "    return newEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01e81dd6",
   "metadata": {
    "executionInfo": {
     "elapsed": 6333,
     "status": "ok",
     "timestamp": 1751171390584,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "618e8e8f"
   },
   "outputs": [],
   "source": [
    "# Eğitilmiş dil modellerinden kelîmeler için katsayıları alma\n",
    "# (bu katsayılar Embedding'e verilerek, Embedding katmanının kayıp üzerindeki etkisi azaltılıyor):\n",
    "if not loadEmbeddingWeightsFromFiles:\n",
    "    weightsOfEmbeddingEncoder = shrinkWordVectors(modelEng, embeddingSize, tokenizerEncoder, asList = True)\n",
    "    weightsOfEmbeddingDecoder = shrinkWordVectors(modelTR, embeddingSize, tokenizerDecoder, asList = True)\n",
    "    # Embedding katmanı için ağırlıkları kaydet:\n",
    "    weightsOfEmbeddingEncoder.tofile(weightsOfEmbeddingEncoderPath)\n",
    "    weightsOfEmbeddingDecoder.tofile(weightsOfEmbeddingDecoderPath)\n",
    "else:\n",
    "    weightsOfEmbeddingEncoder = nm.fromfile(weightsOfEmbeddingEncoderPath, dtype = nm.float64)\n",
    "    sentNum = weightsOfEmbeddingEncoder.shape[0] // embeddingSize\n",
    "    weightsOfEmbeddingEncoder = weightsOfEmbeddingEncoder.reshape((sentNum, embeddingSize))\n",
    "\n",
    "    weightsOfEmbeddingDecoder = nm.fromfile(weightsOfEmbeddingDecoderPath, dtype = nm.float64)\n",
    "    sentNum = weightsOfEmbeddingDecoder.shape[0] // embeddingSize\n",
    "    weightsOfEmbeddingDecoder = weightsOfEmbeddingDecoder.reshape((sentNum, embeddingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36511f7",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1751171322958,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "1ebeffae"
   },
   "outputs": [],
   "source": [
    "# Yaptığımız işlem büyük boyutlu hafîza gerektirdiği için, artık işimiz olmayan dil modellerini hâfızadan silebiliriz:\n",
    "if not loadEmbeddingWeightsFromFiles:\n",
    "    del modelTR, modelEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04270191",
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1751171398127,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "dc2b24d0"
   },
   "outputs": [],
   "source": [
    "# Encoder katmanlarının oluşturulması:\n",
    "encoderInput = Input(shape=(None,), name='encoderInput')\n",
    "encoderEmbedding = Embedding(input_dim=len(tokenizerEncoder.word_index) + 1,\n",
    "                             output_dim=embeddingSize,\n",
    "                            weights=[weightsOfEmbeddingEncoder],\n",
    "                            name = 'encoderEmbedding')\n",
    "\n",
    "encoderGRU1 = GRU(gruCellSize, return_sequences = True, name='encoderGRU1')\n",
    "encoderGRU2 = GRU(gruCellSize, return_sequences = True, name='encoderGRU2')\n",
    "encoderGRU3 = GRU(gruCellSize, return_sequences = False, name='encoderGRU3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e28e7403",
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1751171399237,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "b2af2b39"
   },
   "outputs": [],
   "source": [
    "# Encoder katmanlarının bağlanması:\n",
    "encoder = encoderEmbedding(encoderInput)\n",
    "encoder = encoderGRU1(encoder)\n",
    "encoder = encoderGRU2(encoder)\n",
    "encoder = encoderGRU3(encoder)\n",
    "encoderOutput = encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ed3fd62",
   "metadata": {
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1751171401425,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "1d74c89f"
   },
   "outputs": [],
   "source": [
    "# Decoder katmanlarının oluşturulması:\n",
    "decoderInput = Input(shape=(None,), name = 'decoderInput')\n",
    "decoderEmbedding = Embedding(input_dim = len(tokenizerDecoder.word_index) + 1,\n",
    "                            output_dim = embeddingSize,\n",
    "                            weights=[weightsOfEmbeddingDecoder],\n",
    "                            name = 'decoderEmbedding')\n",
    "decoderGRU1 = GRU(gruCellSize, return_sequences = True, name='decoderGRU1')\n",
    "decoderGRU2 = GRU(gruCellSize, return_sequences = True, name='decoderGRU2')\n",
    "decoderGRU3 = GRU(gruCellSize, return_sequences = True, name='decoderGRU3')\n",
    "# Çıktımız kodlayıcıdaki gibi düşünce vektörü değil, cümle olmalı; bu sebeple son GRU katmanı da dizi döndürmeli\n",
    "# Son katmandan çıkan dizinin, hangi elemana denk düştüğünü anlamalıyız.\n",
    "# Bunun için kelîme sayısı kadar hücre bulunduran bir katman eklemeliyiz:\n",
    "decoderDense = Dense(len(tokenizerDecoder.word_index) + 1, name = 'decoderDense', activation = 'softmax')\n",
    "# Hangi sınıfın değeri daha yüksekse o kelîmenin üretilmesi için 'softmax' aktivasyon seçilmeli;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b12ddf6",
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1751171405969,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "4bef55d6"
   },
   "outputs": [],
   "source": [
    "# Decoder katmanlarının bağlanması:\n",
    "decoder = decoderEmbedding(decoderInput)\n",
    "decoder = decoderGRU1(decoder, initial_state = encoderOutput)\n",
    "decoder = decoderGRU2(decoder, initial_state = encoderOutput)\n",
    "decoder = decoderGRU3(decoder, initial_state = encoderOutput)\n",
    "\n",
    "decoder = decoderDense(decoder)\n",
    "decoderOutput = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77876ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1751171408718,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "855deea8",
    "outputId": "b29bff2e-5c75-421a-c69c-c799128f9c18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoderInput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoderEmbedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,492,800</span> │ encoderInput[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoderGRU1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">175,224</span> │ encoderEmbedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderInput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoderGRU2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">231,672</span> │ encoderGRU1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderEmbedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,183,700</span> │ decoderInput[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoderGRU3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">231,672</span> │ encoderGRU2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderGRU1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">175,224</span> │ decoderEmbedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                               │                           │                 │ encoderGRU3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderGRU2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">231,672</span> │ decoderGRU1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
       "│                               │                           │                 │ encoderGRU3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderGRU3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">231,672</span> │ decoderGRU2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
       "│                               │                           │                 │ encoderGRU3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderDense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81837</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,121,889</span> │ decoderGRU3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoderInput (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoderEmbedding (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │       \u001b[38;5;34m2,492,800\u001b[0m │ encoderInput[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoderGRU1 (\u001b[38;5;33mGRU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)         │         \u001b[38;5;34m175,224\u001b[0m │ encoderEmbedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderInput (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoderGRU2 (\u001b[38;5;33mGRU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)         │         \u001b[38;5;34m231,672\u001b[0m │ encoderGRU1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderEmbedding (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │       \u001b[38;5;34m8,183,700\u001b[0m │ decoderInput[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoderGRU3 (\u001b[38;5;33mGRU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)               │         \u001b[38;5;34m231,672\u001b[0m │ encoderGRU2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderGRU1 (\u001b[38;5;33mGRU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)         │         \u001b[38;5;34m175,224\u001b[0m │ decoderEmbedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                               │                           │                 │ encoderGRU3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderGRU2 (\u001b[38;5;33mGRU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)         │         \u001b[38;5;34m231,672\u001b[0m │ decoderGRU1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
       "│                               │                           │                 │ encoderGRU3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderGRU3 (\u001b[38;5;33mGRU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)         │         \u001b[38;5;34m231,672\u001b[0m │ decoderGRU2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
       "│                               │                           │                 │ encoderGRU3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoderDense (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81837\u001b[0m)       │      \u001b[38;5;34m16,121,889\u001b[0m │ decoderGRU3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,075,525</span> (107.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,075,525\u001b[0m (107.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,075,525</span> (107.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,075,525\u001b[0m (107.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modellerin oluşturulması ve eğitim modelinin derlenmesi:\n",
    "# Birden fazla model oluşturmalıyız; çünkü eğitim modelinde kodlayıcıya iki girdi veriyoruz,\n",
    "# çeviri yaparken bir girdi vermeliyiz\n",
    "\n",
    "modelTrain = Model(inputs=[encoderInput, decoderInput], outputs=[decoderOutput])\n",
    "modelTrain.compile(optimizer = RMSprop(learning_rate = learningRate),\n",
    "                   loss = SparseCategoricalCrossentropy(reduction = 'mean'))\n",
    "\n",
    "modelEncoder = Model(inputs=[encoderInput], outputs=[encoderOutput])\n",
    "modelDecoder = Model(inputs = [decoderInput, encoderOutput], outputs = [decoderOutput])\n",
    "# Modeli ayrı tanımladığımızda, ikinci girdiyi de 'inputs' kısmında belirtmeliyiz.\n",
    "modelTrain.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2dd7972",
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1751171417443,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "e3ebbdd5"
   },
   "outputs": [],
   "source": [
    "# Modeli kaydetmek için 'kayıt için model çağrısı' oluşturmalıyız:\n",
    "saveCheckpoint = ModelCheckpoint(modelSavePath, save_weights_only = True)\n",
    "# Bu yolda bir dosya varsa, o dosyanın üzerine yazılır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "197eb361",
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1751171431318,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "03bb4675"
   },
   "outputs": [],
   "source": [
    "# Model ağırlıklarını dosyadan yükleme (eğer bu ayar açıksa):\n",
    "if loadModelWeights:\n",
    "    modelTrain.load_weights(modelSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e00caf2",
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1751171434012,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "ac047bb2"
   },
   "outputs": [],
   "source": [
    "# Girdi verilerinin tanımlanması:\n",
    "xData = {'encoderInput' : sequencesSrc, 'decoderInput' : sequencesTarget[:,:-1]}\n",
    "yData = sequencesTarget[:,1:]\n",
    "\n",
    "# Kodlayıcı girdi verisi İngilizce cümlenin dizisel hâlidir (Tokenizer + pad_sequences'tan geçmiş)\n",
    "# !!! Kod çözücü girdi verisi son kelîme hâriç, önceki kelîmelerden oluşan dizidir;\n",
    "# !!! Kod çözücü çıktı verisi ilk kelîme hâriç sonraki tüm kelîmelerden oluşan dizidir\n",
    "# Kod çözücü, her adımda sonraki kelîmeyi tahmîn etme üzerine eğitilen modeldir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95fb2ac6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9131620,
     "status": "ok",
     "timestamp": 1751182213638,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "304b30ca",
    "outputId": "9860c296-620f-4e51-ee45-906a06f00824"
   },
   "outputs": [],
   "source": [
    "# Model eğitimi:\n",
    "if trainModel:\n",
    "    epochNum = 6\n",
    "    modelTrain.fit(x = xData, y = yData,\n",
    "                   epochs = epochNum,\n",
    "                   callbacks = [saveCheckpoint],\n",
    "                   batch_size = batchSizeNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f77dc4c",
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1751194004327,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "6457af8d"
   },
   "outputs": [],
   "source": [
    "# Çeviri fonksiyonu:\n",
    "def translate(sentence):\n",
    "    sequences = tokenizerEncoder.sentenceToSequences(sentence, padding = 'pre', reverse = True)# Cümle -> simge dizisi\n",
    "    thoughtMatrix = modelEncoder.predict(sequences)# Düşünce vektörü\n",
    "    maxTokens = tokenizerDecoder.maxToken# Azamî cümle uzunluğu\n",
    "    decoderInput = nm.zeros(shape = (1, maxTokens), dtype = nm.int32)\n",
    "    decoderInput[0][0] = tokenizerDecoder.word_index[startToken.strip()]# Girdinin ilk elemanı başlangıç simgesi olmalı\n",
    "    xData = [decoderInput, thoughtMatrix]# Buradaki sıra, model tanımlanırken verilen sırayla aynı olmalı\n",
    "    result = ''# Üretilen cümle\n",
    "    sayac = 0\n",
    "    while sayac < maxTokens:# Üretim en fazla azamî cümle uzunluğuna kadar devâm etmektedir\n",
    "        vect = modelDecoder.predict(xData)# Model tahmini, kelîme sayısı uzunluğunda bir tek nokta vektörüdür\n",
    "        releatedOneHot = vect[0,sayac,:]# Kelîme için üretilen tek nokta vektörü\n",
    "        token = nm.argmax(releatedOneHot)# Tek nokta vektöründe, değeri 1 olan elemanın indisi, üretilen simgedir.\n",
    "        if token == tokenizerDecoder.word_index[endToken.strip()]:# Bitiş simgesi üretildiyse, kelîme üretmeyi bırak\n",
    "            break\n",
    "        result += ' ' + tokenizerDecoder.tokenToWord(token)\n",
    "        sayac += 1\n",
    "        if sayac != 0 and sayac < maxTokens:\n",
    "            decoderInput[0][sayac] = token# ÖNEMLİ : Sonraki kelîme üretilirken, bu üretilen kelîmeyi girdi olarak vermeliyiz\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "109de6c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "error",
     "timestamp": 1751194009883,
     "user": {
      "displayName": "Mehmet Akif SOLAK",
      "userId": "03652543534396029232"
     },
     "user_tz": -180
    },
    "id": "4eb3fe17",
    "outputId": "0ff07afd-41d1-4d49-c8df-03f4c8024952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaynak metîn : this is a pencil\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Çeviri sonucu :  bu bir kalem\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a pencil\"# Modeli iyi eğittiyseniz, daha uzun ve karışık bir cümle deneyebilirsiniz\n",
    "print(f\"Kaynak metîn : {text}\")\n",
    "print(f\"Çeviri sonucu : {translate(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96a3490a",
   "metadata": {
    "id": "bef85ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaynak metîn : who are you?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "Çeviri sonucu :  sen kimsin\n"
     ]
    }
   ],
   "source": [
    "text = \"who are you?\"# Modeli iyi eğittiyseniz, daha uzun ve karışık bir cümle deneyebilirsiniz\n",
    "print(f\"Kaynak metîn : {text}\")\n",
    "print(f\"Çeviri sonucu : {translate(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dcc8ba",
   "metadata": {
    "id": "fabce9c2"
   },
   "source": [
    "- Eğer modeli 400 bin kelîme, 100 kelîme vektörü ve 200+ GRU hücre sayısı ile 10-15 epoch eğitirseniz 5 - 6 kelîmelik yaygın cümleleriniz doğru şekilde çevrilebilir; fakat veri seti kısa cümlelerden oluştuğundan cümleniz isim cümlesi (noun clause), sıfat cümlesi (noun clause) gibi karmaşık yapıları, sık kullanılmayan kelîmeleri çeviremeyecektir."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
